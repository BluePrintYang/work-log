## 群组版本

### 目的

> 1. 保证消息的顺序性
> 2. 保证消息不丢
> 3. 客户端从服务端拉消息的时机

### 难点

> 1. 如何判断丢消息
> 2. 融云只保存7天离线消息
> 3. 客户端如何判断从服务端拉消息的时机

### 解决方案

#### 1. 消息序列连续递增

> id不连续则丢消息，从服务端拉

##### 实现方案

难度较大

暂未找到能保证连续递增的方案

##### id不连续补偿方案

1. 发消息失败，保留消息，记录消息发送状态（默认发送失败/成功，发送完毕后修改状态）
1. 服务端返回时校验id连续，不连续的直接插入值（可能会丢消息）
1. 发消息成功后，异步查询序列id，不连续则补偿（无法解决消息延迟发送的问题，先拿到id而后发送消息成功）
1. 客户端发现消息id不连续 -> 从服务端拉 -> 服务端为空或不连续 -> 服务端延时补偿
1. 定时任务



![iddd](https://raw.githubusercontent.com/BluePrintYang/PicHub/master/work-log/iddd.png)

##### 问题

连续递增 也可能丢最后一条消息，带上最后一条消息id



#### ~~2. 链式消息序列（记录前后消息id）~~

> ~~链表中断则丢失消息~~

##### ~~实现方案~~

~~发送消息更新前一条消息指针~~

##### ~~问题~~

~~两条消息同时发，前指针指向同一个消息（阻塞？）~~

~~发送失败回滚~~



#### ~~3. 客户端记录消息区间~~

> ~~记录本地最早和最晚的消息id区间 [min, max] 同时记录中间间断的区间~~ 

##### ~~实现方案~~

> ~~服务端新增接口~~

##### ~~问题~~

~~无法解决融云丢失在线消息的问题~~





### 现状

> 针对不同会话，消息序列id连续递增（使用redis）

问题

1. redis宕机服务不可用
2. 先拿到 id 的消息可能滞后发出





#### 在线消息不丢

> 在线消息融云推给客户端，融云来保证在线消息不丢

融云丢消息如何处理？



**im系统是通过应用层的确认，发送方的超时重传，接收方的去重保证业务层面消息的不丢不重。**



#### 离线消息不丢

> 融云只会存储7天的离线消息，大于7天则需要从服务端拉

~~客户端比较最后一条消息与当前时间，大于7天从服务端拉？~~

~~无法解决客户端只拉取一半的问题~~



“离线消息”的可达性可能比大家想象的要复杂，常见的优化有：

（1）对于同一个用户B，一次性拉取所有用户发给ta的离线消息，再在客户端本地进行发送方分析，相比按照发送方一个个进行消息拉取，能大大减少服务器交互次数

（2）分页拉取，先拉取计数再按需拉取，是无线端的常见优化

（3）应用层的ACK，应用层的去重，才能保证离线消息的不丢不重

（4）下一页的拉取，同时作为上一页的ACK，能够极大减少与服务器的交互次数







~~离线消息~~



~~客户端方案：~~

~~记录消息id区间：本地的和间断的~~







### 如何保证IM群聊消息id连续递增

1. 按照接收方收到时序展现
2. 按照服务端收到的时序展
3. 按照发送方发送时序展现







### 其他ID生成策略

#### [融云消息ID生成策略](http://www.52im.net/thread-2747-1-1.html)

> 融云消息数据的唯一 ID 长度采用 80 Bit

每 5 个 Bit ，进行一次 32 进制编码，转换为一个字符。80 Bit 可以转换为 16 个字符，再加上 3 个分隔符（ - ），将 16 个字符分为 4 组，最终得到一个 19 字符的唯一 ID ，形如：*`BD8U-FCOJ-LDC5-L789`*

80 Bit 被分为 4 段

1. **第一段 42 Bit：**用于存放时间戳，最长可表示到 2109 年
2. **第二段 12 Bit：**用于存放自旋转 ID 。自旋 ID 就是在给落到同一毫秒内的消息进行自增编号。12 Bit 则意味着，同一毫秒内，单台主机中最多可以标识 4096（ 2 的 12 次方）条消息。
3. **第三段 4   Bit：**用于标识会话类型。4 Bit ，最多可以标识 16 种会话。
4. **第四段 22 Bit：**会话 ID 。如群聊中的群 ID ，聊天室中的聊天室 ID 等。





#### [美团Leaf-Segment](http://www.52im.net/thread-2751-1-1.html)

##### 基本原理

**数据库表设计如下：**

![image-20220715095604712](https://raw.githubusercontent.com/BluePrintYang/PicHub/master/work-log/image-20220715095604712.png)

> biz_tag：用来区分业务；
> max_id：表示该biz_tag目前所被分配的ID号段的最大值；
> step：表示每次分配的号段长度。

**优点：**

- 1）Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景；
- 2）ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求；
- 3）容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务；
- 4）可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。


**缺点：**

- 1）ID号码不够随机，能够泄露发号数量的信息，不太安全；
- 2）TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺；
- 3）DB宕机会造成整个系统不可用。

##### 双buffer优化

> 号段临界点，从DB取号段。在这期间进来的请求也会因为DB号段没有取回来，导致线程阻塞。假如取DB的时候网络发生抖动，或者DB发生慢查询就会导致整个系统的响应时间变慢。

不需要在DB取号段的时候阻塞请求线程，即当号段消费到某个点时就异步的把下一个号段加载到内存中。而不需要等到号段用尽的时候才去更新号段。这样做就可以很大程度上的降低系统的TP999指标。



#### [百度**UidGenerator**](https://github.com/baidu/uid-generator)

改进后的SnowFlake算法

**原版SnowFlake算法各字段的具体意义是：**

1. 1位sign标识位；
2. 41位时间戳；
3. 10位workId（即5位数据中心id+5位工作机器id）；
4. 12位自增序列。



**UidGenerator默认ID中各数据位的含义如下：**

1. *sign(1bit)：*固定1bit符号标识，即生成的UID为正数。
2. *delta seconds (28 bits)：*当前时间，相对于时间基点"2016-05-20"的增量值，单位：秒，最多可支持约8.7年（注意：(a)这里的单位是秒，而不是毫秒！ (b)注意这里的用词，是“最多”可支持8.7年，为什么是“最多”，后面会讲）。
3. *worker id (22 bits)：*机器id，最多可支持约420w次机器启动。内置实现为在启动时由数据库分配，默认分配策略为用后即弃，后续可提供复用策略。
4. *sequence (13 bits)：*每秒下的并发序列，13 bits可支持每秒8192个并发（注意下这个地方，默认支持qps最大为8192个）



#### [**Tinyid**](http://www.52im.net/thread-3129-1-1.html)

#### [**微信的海量IM聊天消息序列号生成**](http://www.52im.net/thread-1998-1-1.html)

> 每个用户独立的64位 sequence 的体系，而不是用一个全局的64位（或更高位） sequence ，很大原因是全局唯一的 sequence 会有非常严重的申请互斥问题，不容易去实现一个高性能高可靠的架构

> 一个巨大的64位数组，每一个微信用户，都在这个大数组里独占一格8 bytes 的空间，这个格子就放着用户已经分配出去的最后一个 sequence：cur_seq。每个用户来申请sequence的时候，只需要将用户的cur_seq+=1，保存回数组，并返回给用户。

> 只要求递增，并没有要求连续

**于是我们实现了一个简单优雅的策略：**

- 1）内存中储存最近一个分配出去的sequence：cur_seq，以及分配上限：max_seq；
- 2）分配sequence时，将cur_seq++，同时与分配上限max_seq比较：如果cur_seq > max_seq，将分配上限提升一个步长max_seq += step，并持久化max_seq；
- 3）重启时，读出持久化的max_seq，赋值给cur_seq。



重启时要读取大量的max_seq数据加载到内存中

##### 分号段共享存储

> uid 相邻的一段用户属于一个号段，而同个号段内的用户共享一个 max_seq，这样大幅减少了max_seq 数据的大小，同时也降低了IO次数
